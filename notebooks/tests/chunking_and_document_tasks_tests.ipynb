{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab4c1260",
   "metadata": {},
   "source": [
    "# Chunking and Document Task Tests (English & Chinese)\n",
    "This notebook contains unit tests for chunking and document tasks in `cognee/tasks`, covering both English and Chinese text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "13fcc9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "import importlib\n",
    "import sys\n",
    "import asyncio\n",
    "import types\n",
    "\n",
    "async def collect_async_generator(async_gen):\n",
    "    result = []\n",
    "    async for item in async_gen:\n",
    "        result.append(item)\n",
    "    return result\n",
    "\n",
    "def import_task_module(module_path):\n",
    "    return importlib.import_module(module_path)\n",
    "\n",
    "def run_async(coro):\n",
    "    if sys.version_info >= (3, 7):\n",
    "        try:\n",
    "            # For Jupyter, use asyncio.create_task and await\n",
    "            import nest_asyncio\n",
    "            nest_asyncio.apply()\n",
    "            return asyncio.get_event_loop().run_until_complete(coro)\n",
    "        except RuntimeError:\n",
    "            # If event loop is already running (Jupyter), use ensure_future and await\n",
    "            return asyncio.ensure_future(coro)\n",
    "    else:\n",
    "        return asyncio.get_event_loop().run_until_complete(coro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4890ce15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "os.environ['HF_ENDPOINT']='https://hf-mirror.com'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f839aace",
   "metadata": {},
   "source": [
    "## Test 1: Sentence Chunking (English & Chinese)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "72559691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English chunks: [(UUID('55c1d716-f043-4576-92b6-35752fd9cef5'), 'This is a sentence. ', 9, 'sentence_end'), (UUID('55c1d716-f043-4576-92b6-35752fd9cef5'), 'Here is another one! ', 9, 'sentence_end'), (UUID('55c1d716-f043-4576-92b6-35752fd9cef5'), 'And a third?', 6, 'sentence_end')]\n",
      "Chinese chunks: [(UUID('0267e60b-823e-4797-89d4-5ef21a48e99e'), '这是一个句子。', 7, 'sentence_end'), (UUID('0267e60b-823e-4797-89d4-5ef21a48e99e'), '这里还有另一个！', 8, 'sentence_end'), (UUID('0267e60b-823e-4797-89d4-5ef21a48e99e'), '再来一个？', 5, 'sentence_end')]\n"
     ]
    }
   ],
   "source": [
    "def test_chunk_by_sentence_english_and_chinese():\n",
    "    import asyncio\n",
    "    chunk_by_sentence = import_task_module('cognee.tasks.chunks.chunk_by_sentence')\n",
    "    # English\n",
    "    english_text = \"This is a sentence. Here is another one! And a third?\"\n",
    "    english_chunks_result = chunk_by_sentence.chunk_by_sentence(english_text)\n",
    "    if asyncio.iscoroutine(english_chunks_result):\n",
    "        english_chunks = asyncio.get_event_loop().run_until_complete(english_chunks_result)\n",
    "    else:\n",
    "        english_chunks = list(english_chunks_result)\n",
    "    print(f\"English chunks: {english_chunks}\")\n",
    "\n",
    "    # Chinese\n",
    "    chinese_text = \"这是一个句子。这里还有另一个！再来一个？\"\n",
    "    chinese_chunks_result = chunk_by_sentence.chunk_by_sentence(chinese_text)\n",
    "    if asyncio.iscoroutine(chinese_chunks_result):\n",
    "        chinese_chunks = asyncio.get_event_loop().run_until_complete(chinese_chunks_result)\n",
    "    else:\n",
    "        chinese_chunks = list(chinese_chunks_result)\n",
    "    print(f\"Chinese chunks: {chinese_chunks}\")\n",
    "\n",
    "\n",
    "test_chunk_by_sentence_english_and_chinese()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb7b513",
   "metadata": {},
   "source": [
    "## Test 2: Word and Paragraph Chunking (English & Chinese)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "027e68b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English chunks: [('Hello ', 'word'), ('world! ', 'sentence_end'), ('This ', 'word'), ('is ', 'word'), ('a ', 'word'), ('test.', 'sentence_end')] \n",
      "[('你好世界！', 'sentence_end'), ('这是一个测试。', 'sentence_end')]\n",
      "English chunks: [{'text': 'Paragraph one.', 'chunk_size': 5, 'chunk_id': UUID('8c1cda85-52f4-508b-a2f4-ce77ebaa528a'), 'paragraph_ids': [UUID('b796e2a4-c26e-4ee2-b54a-be716a1e0eac')], 'chunk_index': 0, 'cut_type': 'paragraph_end'}, {'text': '\\n\\nParagraph two.', 'chunk_size': 8, 'chunk_id': UUID('16539612-e6fc-5eb2-bd2b-779d034c397a'), 'paragraph_ids': [UUID('b796e2a4-c26e-4ee2-b54a-be716a1e0eac')], 'chunk_index': 1, 'cut_type': 'sentence_end'}] \n",
      "Chinese chunks: [{'text': '第一段。', 'chunk_size': 5, 'chunk_id': UUID('e573efab-6951-5670-88e1-ba632e8d2eea'), 'paragraph_ids': [UUID('d0448a55-23c7-42cc-bd1d-d824193c9221')], 'chunk_index': 0, 'cut_type': 'paragraph_end'}, {'text': '\\n\\n第二段。', 'chunk_size': 7, 'chunk_id': UUID('6696819a-592e-5694-abff-c84b9c415fe9'), 'paragraph_ids': [UUID('d0448a55-23c7-42cc-bd1d-d824193c9221')], 'chunk_index': 1, 'cut_type': 'sentence_end'}] \n"
     ]
    }
   ],
   "source": [
    "def test_chunk_by_word_english_and_chinese():\n",
    "    chunk_by_word = import_task_module('cognee.tasks.chunks.chunk_by_word')\n",
    "    english_text = \"Hello world! This is a test.\"\n",
    "    chinese_text = \"你好世界！这是一个测试。\"\n",
    "    english_chunks = chunk_by_word.chunk_by_word(english_text)\n",
    "    chinese_chunks = chunk_by_word.chunk_by_word(chinese_text)\n",
    "    print(f\"English chunks: { list( english_chunks)} \")\n",
    "    print(list(chinese_chunks))\n",
    "\n",
    "def test_chunk_by_paragraph_english_and_chinese():\n",
    "    chunk_by_paragraph = import_task_module('cognee.tasks.chunks.chunk_by_paragraph')\n",
    "    english_text = \"Paragraph one.\\n\\nParagraph two.\"\n",
    "    chinese_text = \"第一段。\\n\\n第二段。\"\n",
    "    english_chunks = chunk_by_paragraph.chunk_by_paragraph(english_text, max_chunk_size=10)\n",
    "    chinese_chunks = chunk_by_paragraph.chunk_by_paragraph(chinese_text, max_chunk_size=10)\n",
    "    print(f\"English chunks: { list( english_chunks)} \")\n",
    "    print(f\"Chinese chunks: { list( chinese_chunks)} \")\n",
    "\n",
    "test_chunk_by_word_english_and_chinese()\n",
    "test_chunk_by_paragraph_english_and_chinese()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
